\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PACKAGE IMPORTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[tmargin=2cm,rmargin=1in,lmargin=1in,margin=0.85in,bmargin=2cm,footskip=.2in]{geometry}
\usepackage{tikz}
\usepackage{amsmath,amsfonts,amsthm,amssymb,mathtools}
%\usepackage[varbb]{newpxmath}
%\usepackage{xfrac}
%\usepackage[makeroom]{cancel}
%\usepackage{mathtools}
%\usepackage{bookmark}
%\usepackage{enumitem}
%\usepackage{hyperref,theoremref}

\begin{document}
\title{Neural Network Notes}
\author{Mimanshu Maheshwari}

\section{Gradient Descent}
As the $\epsilon$ decrease in Finite Difference approach result is derivative of cost function.
\begin{align}
	C'(w) &= \lim_{\epsilon \to 0}\frac{C(w + \epsilon) - C(w)}{\epsilon}
\end{align}

\subsection{Linier Model}


\end{document}
