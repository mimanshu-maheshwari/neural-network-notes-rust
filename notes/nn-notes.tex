\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PACKAGE IMPORTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[tmargin=2cm,rmargin=1in,lmargin=1in,margin=0.85in,bmargin=2cm,footskip=.2in]{geometry}
\usepackage{tikz}
\usepackage{amsmath,amsfonts,amsthm,amssymb,mathtools}
%\usepackage[varbb]{newpxmath}
%\usepackage{xfrac}
%\usepackage[makeroom]{cancel}
%\usepackage{mathtools}
%\usepackage{bookmark}
%\usepackage{enumitem}
%\usepackage{hyperref,theoremref}

\begin{document}
\title{Neural Network Notes}
\author{Mimanshu Maheshwari}


\section{Finite Differences}
As the $\epsilon$ decrease in Finite Difference approach result is derivative of cost function.
\begin{align}
	C'(w) &= \lim_{\epsilon \to 0}\frac{C(w + \epsilon) - C(w)}{\epsilon}
\end{align}

\section{Cost function}
Cost function is used to calculate the output of the network and using the output we can calculate how accurate the function is? \\
Cost function for a single layer can be written as:
\subsection{Cost Function for single neuron}
\begin{align}
	C(w) &= \frac{1}{n}\sum_{i=1}^{n}{\left(x_iw - y_i\right)^2} \\
	C'(w) &= \frac{1}{n}\sum_{i=1}^{n}{2x\left(k_iw - y_i\right)}
\end{align}




\end{document}
