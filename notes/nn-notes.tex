\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PACKAGE IMPORTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[tmargin=2cm,rmargin=1in,lmargin=1in,margin=0.85in,bmargin=2cm,footskip=.2in]{geometry}
\usepackage{tikz}
\usepackage{amsmath,amsfonts,amsthm,amssymb,mathtools}
%\usepackage[varbb]{newpxmath}
%\usepackage{xfrac}
%\usepackage[makeroom]{cancel}
%\usepackage{mathtools}
%\usepackage{bookmark}
%\usepackage{enumitem}
%\usepackage{hyperref,theoremref}

\begin{document}
\title{Neural Network Notes}
\author{Mimanshu Maheshwari}


\section{Gradient Descent}
As the $\epsilon$ decrease in Finite Difference approach result is derivative of cost function.
\begin{align}
	C'(w) &= \lim_{\epsilon \to 0}\frac{C(w + \epsilon) - C(w)}{\epsilon}
\end{align}

\subsection{Cost function}
Cost function is used to calculate the output of the network and using the output we can calculate how accurate the function is? \\
Cost function for a single layer can be written as:
\begin{align}
	C &= \sum_{i = 1}^{n}{a_iw_i + b}
\end{align}

So for a layer cost function becomes: 
\begin{align}
	C^{l} = \sum_{i = 1}^{n}{a_{i}^{l}w_{i}^{l - 1} + b^{l}}
\end{align}

\subsection{Linier Model}


\end{document}
